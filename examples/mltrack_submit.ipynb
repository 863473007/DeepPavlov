{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit for MLTrack 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed to get the submit for MLTrack 2018 competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import key components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov import build_model\n",
    "from deeppavlov.core.commands.utils import parse_config\n",
    "from deeppavlov.core.commands.train import read_data_by_config, train_evaluate_model_from_config, get_iterator_from_config\n",
    "from deeppavlov.download import deep_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a config file for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CONFIG = '../deeppavlov/configs/ranking/mltrack_ranker.json'\n",
    "config = parse_config(PATH_TO_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and decompress required files including mltrack dataset, pretrained Bert weights and fine-tuned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_download(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to start training, then use the following cell. Otherwise, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_model_from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't train model in the previous step and want just to use fine-tuned weights, change load_path in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['chainer']['pipe'][1]['load_path'] = '{DOWNLOADS_PATH}/fine_tuned_model/mltrack_model_fine_tuned/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get probabilities of each class instead of labels for every answer in test dataset, change config file. It is needed in order to do more accurate ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['chainer']['pipe'][1]['return_probas'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model based on the prepared config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(config, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and parse data from files using dataset reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_by_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches with the help of dataset iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = get_iterator_from_config(config, data)\n",
    "batches = [x for x in iterator.gen_batches(batch_size=36, data_type='test', shuffle=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is prepared for the prediction. Just run the following cell to get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch in batches:\n",
    "    predictions.extend(model(batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of predictions contains probability distribution for each answer. Now let's prepare context ids for writing in submit file. Splitting is needed to determine the number of answers for every context (sometimes it is not equal to 6) and then to remove padded answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOR_SUBMIT_PATH = os.path.expanduser(Path(config[\"dataset_reader\"][\"data_path\"]) / Path('final.tsv'))\n",
    "\n",
    "with open(DATA_FOR_SUBMIT_PATH, \"r\") as f:\n",
    "    data_for_ids = f.readlines()    \n",
    "    \n",
    "data_for_ids = [el.strip('\\n').split('\\t') for el in data_for_ids]\n",
    "context_ids = [el[0] for el in data_for_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_context_ids(context_id):\n",
    "    splitted_context_id, cur_ids = [], []\n",
    "    cur_id = context_id[0]\n",
    "    for el in context_id:\n",
    "        if el == cur_id:\n",
    "            cur_ids.append(el)\n",
    "        else:\n",
    "            splitted_context_id.append(cur_ids)\n",
    "            cur_id = el\n",
    "            cur_ids = [cur_id]\n",
    "    splitted_context_id.append(cur_ids)\n",
    "    return splitted_context_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_context_ids = split_context_ids(context_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save probabilities only for real answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_for_real_data = [pred[:len(ids)] for pred, ids in zip(predictions, splitted_context_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final score of an answer is positive if probability of 'good' class has highest value. It is negative for 'bad' answers and is equal to zero for 'neutral' answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs = [np.argmax(el, -1)-1 for el in pred_for_real_data]\n",
    "scores = [np.max(el, -1) for el in pred_for_real_data]\n",
    "pred_for_real_data = [sign*score for sign, score in zip(signs, scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using obtained scores let's get ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = [np.flip(np.argsort(el), -1) for el in pred_for_real_data] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save submit file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = [i+' '+str(el) for ids, rank in zip(splitted_context_ids, ranking) for i, el in zip(ids, rank)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.expanduser(config[\"metadata\"][\"variables\"][\"ROOT_PATH\"])+\"/submit.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(submit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find submit file in a directory `~/.deeppavlov`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
